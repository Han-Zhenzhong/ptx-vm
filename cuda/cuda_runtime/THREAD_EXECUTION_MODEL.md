# PTX VM å¤šçº¿ç¨‹æ‰§è¡Œæ¨¡å‹è¯´æ˜

## ğŸ¯ æ ¸å¿ƒé—®é¢˜
å½“ `cudaLaunchKernel` å¯åŠ¨ä¸€ä¸ª kernel æ—¶ï¼Œgrid å’Œ block é…ç½®æŒ‡å®šäº†æˆç™¾ä¸Šåƒçš„çº¿ç¨‹ï¼ŒPTX VM å¦‚ä½•æ¨¡æ‹Ÿè¿™äº›çº¿ç¨‹çš„å¹¶è¡Œæ‰§è¡Œï¼Ÿ

---

## ğŸ—ï¸ PTX VM çš„æ‰§è¡Œæ¶æ„

### SIMT æ‰§è¡Œæ¨¡å‹ (Single Instruction Multiple Threads)

PTX VM å®ç°äº† GPU çš„ SIMT æ‰§è¡Œæ¨¡å‹ï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼æ¨¡æ‹Ÿå¤šçº¿ç¨‹å¹¶è¡Œï¼š

```
Grid (æ‰€æœ‰çº¿ç¨‹)
  â””â”€â”€ å¤šä¸ª Block (CTA - Cooperative Thread Array)
       â””â”€â”€ å¤šä¸ª Warp (32ä¸ªçº¿ç¨‹ä¸€ç»„)
            â””â”€â”€ 32ä¸ª Thread å¹¶è¡Œæ‰§è¡Œç›¸åŒæŒ‡ä»¤
```

### å…³é”®ç»„ä»¶

#### 1. **Warp Scheduler (src/execution/warp_scheduler.cpp)**
è´Ÿè´£ç®¡ç†å’Œè°ƒåº¦ warp çš„æ‰§è¡Œï¼š

```cpp
class WarpScheduler {
    uint32_t m_numWarps;            // warp æ•°é‡
    uint32_t m_threadsPerWarp;      // æ¯ä¸ª warp çš„çº¿ç¨‹æ•°ï¼ˆé€šå¸¸32ï¼‰
    std::vector<Warp*> m_warps;     // æ‰€æœ‰ warp å¯¹è±¡
    uint32_t m_currentWarp;         // å½“å‰æ‰§è¡Œçš„ warp
}
```

#### 2. **Warp å¯¹è±¡**
è¡¨ç¤ºä¸€ç»„å¹¶è¡Œæ‰§è¡Œçš„çº¿ç¨‹ï¼š

```cpp
class Warp {
    uint32_t m_warpId;              // Warp ID
    uint32_t m_numThreads;          // çº¿ç¨‹æ•°é‡
    uint64_t m_activeMask;          // æ´»è·ƒçº¿ç¨‹æ©ç ï¼ˆ64ä½ï¼Œæ¯ä½ä»£è¡¨ä¸€ä¸ªçº¿ç¨‹ï¼‰
    size_t m_currentPC;             // å½“å‰ç¨‹åºè®¡æ•°å™¨
    std::vector<size_t> m_threadPCs; // æ¯ä¸ªçº¿ç¨‹çš„ PCï¼ˆç”¨äºåˆ†æ”¯åˆ†æ­§ï¼‰
}
```

---

## ğŸ”„ æ‰§è¡Œæµç¨‹

### 1. å†…æ ¸å¯åŠ¨ (cudaLaunchKernel)

```cpp
cudaLaunchKernel(func, 
    dim3(blocksPerGrid),      // Grid: 4 blocks
    dim3(threadsPerBlock),    // Block: 256 threads
    args, 0, nullptr);
```

**è®¡ç®—æ€»çº¿ç¨‹æ•°**ï¼š
- Total threads = gridDim Ã— blockDim = 4 Ã— 256 = 1024 threads
- Warps needed = 1024 / 32 = 32 warps

### 2. PTX VM åˆå§‹åŒ– (åœ¨ HostAPI::cuLaunchKernel ä¸­)

```cpp
// åœ¨ host_api.cpp ä¸­
CUresult cuLaunchKernel(...) {
    // 1. è®¡ç®— warp æ•°é‡
    uint32_t totalThreads = gridDimX * gridDimY * gridDimZ * 
                            blockDimX * blockDimY * blockDimZ;
    uint32_t numWarps = (totalThreads + 31) / 32;
    
    // 2. åˆå§‹åŒ– WarpScheduler
    WarpScheduler& scheduler = executor.getWarpScheduler();
    scheduler.initialize(numWarps, 32);
    
    // 3. è®¾ç½®æ¯ä¸ª warp çš„åˆå§‹çŠ¶æ€
    for (uint32_t warpId = 0; warpId < numWarps; ++warpId) {
        scheduler.setActiveThreads(warpId, 0xFFFFFFFF);  // æ‰€æœ‰çº¿ç¨‹æ¿€æ´»
        scheduler.setCurrentPC(warpId, 0);               // ä» PC=0 å¼€å§‹
    }
    
    // 4. æ‰§è¡Œå†…æ ¸
    bool success = m_vm->run();
}
```

### 3. æ‰§è¡Œå¾ªç¯ (åœ¨ PTXVM::run ä¸­)

```cpp
bool PTXVM::run() {
    WarpScheduler& scheduler = getWarpScheduler();
    
    // ä¸»æ‰§è¡Œå¾ªç¯
    while (!scheduler.allWarpsComplete()) {
        // é€‰æ‹©ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„ warpï¼ˆè½®è¯¢è°ƒåº¦ï¼‰
        uint32_t warpId = scheduler.selectNextWarp();
        
        if (!scheduler.warpHasWork(warpId)) {
            continue;
        }
        
        // è·å–å½“å‰ warp çš„çŠ¶æ€
        uint64_t activeMask = scheduler.getActiveThreads(warpId);
        size_t currentPC = scheduler.getCurrentPC(warpId);
        
        // å–æŒ‡ä»¤
        DecodedInstruction instr = fetchInstruction(currentPC);
        
        // æ‰§è¡ŒæŒ‡ä»¤ï¼ˆæ‰€æœ‰æ´»è·ƒçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œï¼‰
        executeInstruction(instr, warpId, activeMask);
        
        // æ›´æ–° PC
        scheduler.setCurrentPC(warpId, currentPC + 1);
    }
    
    return true;
}
```

---

## ğŸ§µ çº¿ç¨‹å¹¶è¡Œçš„å®ç°æ–¹å¼

### æ–¹å¼ 1: é€»è¾‘å¹¶è¡Œï¼ˆå½“å‰å®ç°ï¼‰

PTX VM ä½¿ç”¨ **ä¸²è¡Œæ¨¡æ‹Ÿå¹¶è¡Œ** çš„æ–¹å¼ï¼š

```cpp
void executeInstruction(DecodedInstruction& instr, uint32_t warpId, uint64_t activeMask) {
    RegisterBank& regBank = getRegisterBank();
    
    // éå† warp ä¸­çš„æ¯ä¸ªçº¿ç¨‹
    for (uint32_t threadId = 0; threadId < 32; ++threadId) {
        // æ£€æŸ¥è¯¥çº¿ç¨‹æ˜¯å¦æ´»è·ƒ
        if (!(activeMask & (1ULL << threadId))) {
            continue;  // è·³è¿‡éæ´»è·ƒçº¿ç¨‹
        }
        
        // è®¾ç½®çº¿ç¨‹ IDï¼ˆç”¨äº %tid, %ntid ç­‰ç‰¹æ®Šå¯„å­˜å™¨ï¼‰
        regBank.setThreadId(threadId, 0, 0);
        
        // æ‰§è¡ŒæŒ‡ä»¤ï¼ˆé’ˆå¯¹è¿™ä¸ªçº¿ç¨‹ï¼‰
        switch (instr.opcode) {
            case PTXOpcode::ADD:
                // è¯»å–æºå¯„å­˜å™¨
                uint64_t src1 = regBank.read(instr.src1, threadId);
                uint64_t src2 = regBank.read(instr.src2, threadId);
                // æ‰§è¡Œè®¡ç®—
                uint64_t result = src1 + src2;
                // å†™å…¥ç›®æ ‡å¯„å­˜å™¨
                regBank.write(instr.dest, result, threadId);
                break;
            // ... å…¶ä»–æŒ‡ä»¤
        }
    }
}
```

**å…³é”®ç‚¹**ï¼š
- æ¯ä¸ª warp çš„ 32 ä¸ªçº¿ç¨‹ **é€ä¸ªä¸²è¡Œæ‰§è¡Œ**
- ä½†é€»è¾‘ä¸Šæ˜¯ **åŒæ—¶æ‰§è¡ŒåŒä¸€æ¡æŒ‡ä»¤**
- é€šè¿‡ `activeMask` æ§åˆ¶å“ªäº›çº¿ç¨‹å®é™…æ‰§è¡Œ

### æ–¹å¼ 2: ç‰©ç†å¹¶è¡Œï¼ˆå¯èƒ½çš„ä¼˜åŒ–ï¼‰

PTX VM **å¯ä»¥** ä½¿ç”¨ C++ çº¿ç¨‹æ± è¿›è¡ŒçœŸæ­£çš„å¹¶è¡Œï¼š

```cpp
void executeInstructionParallel(DecodedInstruction& instr, uint32_t warpId, uint64_t activeMask) {
    std::vector<std::thread> threads;
    
    // ä¸ºæ¯ä¸ªæ´»è·ƒçº¿ç¨‹åˆ›å»ºå®é™…çš„ OS çº¿ç¨‹
    for (uint32_t threadId = 0; threadId < 32; ++threadId) {
        if (activeMask & (1ULL << threadId)) {
            threads.emplace_back([&, threadId]() {
                // åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œ
                executeForThread(instr, warpId, threadId);
            });
        }
    }
    
    // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for (auto& t : threads) {
        t.join();
    }
}
```

**æ³¨æ„**ï¼šå½“å‰ PTX VM **ä¼¼ä¹ä½¿ç”¨ä¸²è¡Œæ¨¡æ‹Ÿ**ï¼Œè¿™å¯¹äºè°ƒè¯•å’Œç®€å•åœºæ™¯å·²è¶³å¤Ÿã€‚

---

## ğŸ”€ åˆ†æ”¯åˆ†æ­§ (Divergence) å¤„ç†

å½“ warp ä¸­çš„çº¿ç¨‹é‡åˆ°åˆ†æ”¯æ—¶ï¼š

```ptx
@p setp.eq.s32 p, %tid.x, 0    // åªæœ‰çº¿ç¨‹0è®¾ç½®è°“è¯p
@p bra target                   // åªæœ‰pä¸ºtrueçš„çº¿ç¨‹è·³è½¬
```

**å¤„ç†æ–¹å¼**ï¼š

```cpp
void handleBranchDivergence(uint32_t warpId, uint64_t takenMask, 
                           size_t targetPC, size_t fallthroughPC) {
    Warp& warp = getWarp(warpId);
    
    // 1. ä¿å­˜åˆ†æ­§ç‚¹åˆ°å †æ ˆ
    DivergenceStackEntry entry;
    entry.reconvergencePC = fallthroughPC;
    entry.activeMask = ~takenMask;  // æœªè·³è½¬çš„çº¿ç¨‹
    warp.pushDivergence(entry);
    
    // 2. æ›´æ–°æ´»è·ƒæ©ç ï¼ˆåªæ‰§è¡Œè·³è½¬çš„çº¿ç¨‹ï¼‰
    warp.setActiveMask(takenMask);
    warp.setCurrentPC(targetPC);
    
    // 3. å½“è·³è½¬è·¯å¾„æ‰§è¡Œå®Œï¼Œæ¢å¤åˆ° reconvergencePC
    //    åˆå¹¶ä¸¤æ¡è·¯å¾„ï¼Œç»§ç»­æ‰§è¡Œ
}
```

---

## ğŸ“ åœ¨ libptxrt ä¸­çš„å½±å“

### å½“å‰å®ç°

```cpp
// cuda_runtime.cpp
cudaError_t cudaLaunchKernel(const void* func, dim3 gridDim, dim3 blockDim,
                             void** args, size_t sharedMem, cudaStream_t stream) {
    // ...
    
    // è°ƒç”¨ HostAPI
    CUresult result = state.host_api.cuLaunchKernel(
        f,
        gridDim.x, gridDim.y, gridDim.z,
        blockDim.x, blockDim.y, blockDim.z,
        sharedMem,
        nullptr,
        args,
        nullptr
    );
    
    // PTX VM å†…éƒ¨ä¼šï¼š
    // 1. è®¡ç®— warp æ•°é‡
    // 2. åˆå§‹åŒ– WarpScheduler
    // 3. ä¸²è¡Œæ¨¡æ‹Ÿæ‰€æœ‰ warp çš„å¹¶è¡Œæ‰§è¡Œ
    // 4. è¿”å›ç»“æœ
}
```

### éœ€è¦æ³¨æ„çš„ç‚¹

1. **æ‰§è¡Œæ˜¯åŒæ­¥çš„**ï¼š`m_vm->run()` ä¼šé˜»å¡ç›´åˆ°æ‰€æœ‰çº¿ç¨‹å®Œæˆ
2. **æ²¡æœ‰çœŸæ­£çš„å¹¶è¡Œ**ï¼šçº¿ç¨‹æ˜¯é€ä¸ªæ‰§è¡Œçš„ï¼ˆé™¤é PTX VM å†…éƒ¨ä½¿ç”¨äº†çº¿ç¨‹æ± ï¼‰
3. **å†…å­˜è®¿é—®æ˜¯ä¸²è¡Œçš„**ï¼šä¸ä¼šæœ‰çœŸæ­£çš„ç«æ€æ¡ä»¶
4. **æ€§èƒ½ç‰¹å¾ä¸åŒ**ï¼š
   - GPUï¼šçœŸæ­£çš„å¹¶è¡Œï¼Œå—å†…å­˜å¸¦å®½é™åˆ¶
   - PTX VMï¼šä¸²è¡Œæ¨¡æ‹Ÿï¼Œå— CPU å•æ ¸æ€§èƒ½é™åˆ¶

---

## ğŸ“ æ€»ç»“

### PTX VM å¦‚ä½•æ¨¡æ‹Ÿå¤šçº¿ç¨‹ï¼Ÿ

1. **é€»è¾‘ç»“æ„**ï¼š
   - Grid â†’ Block â†’ Warp â†’ Thread çš„å±‚æ¬¡ç»“æ„
   - æ¯ä¸ª Warp åŒ…å« 32 ä¸ªé€»è¾‘çº¿ç¨‹

2. **æ‰§è¡Œæ–¹å¼**ï¼š
   - **ä¸²è¡Œæ¨¡æ‹Ÿå¹¶è¡Œ**ï¼šä¸€æ¬¡æ‰§è¡Œä¸€ä¸ª warp çš„ä¸€æ¡æŒ‡ä»¤
   - é€šè¿‡ `activeMask` æ§åˆ¶å“ªäº›çº¿ç¨‹æ‰§è¡Œ
   - é€šè¿‡ç‰¹æ®Šå¯„å­˜å™¨ï¼ˆ%tid.x, %ntid.x ç­‰ï¼‰è®©æ¯ä¸ªçº¿ç¨‹çŸ¥é“è‡ªå·±çš„ ID

3. **è°ƒåº¦**ï¼š
   - WarpScheduler ä½¿ç”¨è½®è¯¢ï¼ˆround-robinï¼‰è°ƒåº¦ warp
   - å¤„ç†åˆ†æ”¯åˆ†æ­§å’Œé‡æ–°æ±‡èš
   - ç®¡ç†åŒæ­¥åŸè¯­ï¼ˆ__syncthreadsï¼‰

4. **å¯¹ libptxrt çš„å½±å“**ï¼š
   - `cudaLaunchKernel` æ˜¯**åŒæ­¥çš„**ï¼Œä¼šç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
   - ä¸éœ€è¦é¢å¤–çš„çº¿ç¨‹ç®¡ç†æˆ–åŒæ­¥
   - å‚æ•°ä¼ é€’ç»™æ‰€æœ‰çº¿ç¨‹ï¼ˆé€šè¿‡å‚æ•°å†…å­˜ï¼‰

### ç¤ºä¾‹æ‰§è¡Œæµç¨‹

```
cudaLaunchKernel(kernel, dim3(2), dim3(64), args)
  â†“
totalThreads = 2 Ã— 64 = 128
numWarps = 128 / 32 = 4
  â†“
For each instruction in kernel:
    For warpId = 0 to 3:
        For threadId = 0 to 31:
            if thread is active:
                Execute instruction for this thread
  â†“
All warps complete
  â†“
Return cudaSuccess
```

---

## ğŸ” éªŒè¯æ–¹æ³•

å¯ä»¥æ·»åŠ è°ƒè¯•æ—¥å¿—æ¥è§‚å¯Ÿæ‰§è¡Œï¼š

```cpp
// åœ¨ cuda_runtime.cpp ä¸­
cudaError_t cudaLaunchKernel(...) {
    printf("[libptxrt] Total threads: %u Ã— %u Ã— %u = %u\n",
           gridDim.x * blockDim.x, gridDim.y * blockDim.y, gridDim.z * blockDim.z,
           gridDim.x * blockDim.x * gridDim.y * blockDim.y * gridDim.z * blockDim.z);
    printf("[libptxrt] Estimated warps: %u\n",
           (gridDim.x * blockDim.x * gridDim.y * blockDim.y * gridDim.z * blockDim.z + 31) / 32);
    
    // ... è°ƒç”¨ HostAPI
}
```

è¿™æ ·è¿è¡Œ simple_add æ—¶å°±èƒ½çœ‹åˆ°ï¼š
```
[libptxrt] Total threads: 1024
[libptxrt] Estimated warps: 32
[libptxrt] Launching kernel...
```

---

**å‚è€ƒæ–‡æ¡£**ï¼š
- `src/execution/warp_scheduler.cpp` - Warp è°ƒåº¦å®ç°
- `docs_spec/warp_scheduler.md` - Warp è°ƒåº¦å™¨è§„èŒƒ
- `src/host/host_api.cpp` - å†…æ ¸å¯åŠ¨å®ç°
